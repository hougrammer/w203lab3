---
title: "Lab 3"
author: "David Hou, Scott Hungerfield, Irene Seo"
date: "March 20, 2018"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, include = TRUE, warning = FALSE, message = FALSE, dev = 'pdf', fig.height = 4)
library(dplyr)
library(ggplot2)
library(GGally)
library(lmtest)
library(sandwich)
library(stargazer)
```

# Introduction

The purpose of this study is to provide information for political campaign in North Carolina.  Specifically, we want to determine what variables contribute to crime rate and help the campaign propose policy suggestions to local governments.  To accomplish this, we were given crime data from several North Carolina counties along with other variables.  We will run ordinary least square regressions to help determine which of these are the best predictors of crime.

# Data Cleaning

First we need to clean the data.  In the raw data, we notice that that the last 6 rows are empty.  The integer columns are probably more useful to us as factors.  The prbconv is coded as a factor, so we turn it into a numeric.

We also notice that prbarr and prbconv have values that are greater than 1, which does not make much sense because they are probability variables.  We assume that these values were coded incorrectly and filter those out.

As a minor change, we divide pctmin80 by 100, so that it matches the formatting of pctymle.  Both variables are percentages and we've arbitrarily chosen to represent them as a number between 0 and 1 rather than 0 to 100.
```{r data_cleaning}
raw = as_tibble(read.csv('crime_v2.csv'))
t = raw %>% 
    filter(!is.na(county)) %>%
    mutate(prbconv = as.numeric(as.character(prbconv))) %>%
    mutate(pctmin80 = pctmin80 / 100) %>%
    mutate_if(is.integer, as.factor) %>%
    filter(prbarr < 1 & prbconv < 1)
levels(t$west) = c('East', 'West')
t$west = relevel(t$west, 'West') # Put West first so it appears on the left on facet plots 
levels(t$central) = c('Outer', 'Central')
levels(t$urban) = c('Non-urban', 'Urban')
```



Here is a summary of the data.
```{r summary, results='asis'}
stargazer(data.frame(t), type = 'latex', nobs = FALSE, header = FALSE, float = FALSE)
```

# Examining Key Variables of Interest

## Metric Variables

We start our analysis by first looking at the metric variables, i.e. all the variables less county, year, west, central, and urban.  Crime rate is our most important variable as it is the output that we are trying to study.

```{r crmrte_hist, fig.height=3}
qplot(t$crmrte, col = I('white')) + 
    labs(title = 'Crime Rate', x = 'Crimes Committed per Person')
summary(t$crmrte)
```

We see that crime rate has some positive skew, but does not seem to have a very exotic distribution.  To determine which variables are of interest to us when predicting crime rate, we look at the correlation matrices among the variables.  First, let us treat the wage variables by themselves.

```{r wage_cor, fig.height=5}
ggcorr(t %>% select(crmrte, wcon, wtuc, wtrd, wfir, wser, wmfg, wfed, wsta, wloc),
       label = TRUE, label_round = 2, label_size = 3, size = 3) + 
    ggtitle('Correlation Matrix of Crime Rate and Wages')
```
Surprisingly, we find that crime rate is actually positively correlated with all wages.  This seems counter to common sentiment that crime is more prevalent in low income areas.  Interestingly, we notice federal wages being the most correlated with crime rate (r = `r round(cor(t$crmrte, t$wfed), 2)`) and state wages being the least correlated (r = `r round(cor(t$crmrte, t$wsta), 2)`).

For ease of comparison with the other variables, we create a new one that is the sum of all the other wages.  We will see later that this data transformation does not make a large difference in the regression analysis.

```{r correlation_matrix, fig.height=5}
t = t %>% mutate(wage = wcon + wtuc + wtrd + wfir + wser + wmfg + wfed + wsta + wloc)

ggcorr(t %>% select(crmrte, prbarr, prbconv, prbpris, avgsen, polpc, density, taxpc, pctmin80, mix, pctymle, wage),
       label = TRUE, label_round = 2, label_size = 3, size = 3) + 
    ggtitle('Correlation Matrix')
```

From the correlation matrix, we see that population density stands out as being highly correlated with crime rate (r = `r round(cor(t$crmrte, t$density), 2)`).  This variable looks like a good candidate as a predictor for crime rate.  One explanation could be that as more people move into an area, the increased number of interactions give opportunity for more crime.  In addition, more people in an area probably increases the chance that crime will actually be seen.

```{r crmrte_vs_density}
qplot(t$density, t$crmrte) +  
    labs(title = 'Crime Rate vs Population Density', x = 'People per Square Mile', y = 'Crimes Committed per Person') + 
    geom_smooth(method = 'lm', se = FALSE)
```

The other two variables with moderately positive correlation are tax per capita (r = `r round(cor(t$crmrte, t$taxpc), 2)`) and total wages (r = `r round(cor(t$crmrte, t$wage), 2)`).  It is interesting to note that taxes and wages are not very correlated with themselves (r = `r round(cor(t$taxpc, t$wage), 2)`).  This finding is surprising, as one would expect that wages and taxes would go up very closely with each other.  Also note that population density is weakly correlated with taxes (r = `r round(cor(t$taxpc, t$density), 2)`) and moderately correlated with wages (r = `r round(cor(t$wage, t$density), 2)`).  We believe that taxes and wages are not directly causing higher crime rates but are rising along with crime rate because they are rising along with density.


```{r crmrte_vs_taxpc}
qplot(t$taxpc, t$crmrte) + 
    labs(title = 'Crime Rate vs Taxes', x = 'Tax Revenue per Capita', y = 'Crimes Committed per Person') +
    geom_smooth(method = 'lm', se = FALSE)
```
```{r crmrte_vs_wage}
qplot(t$wage, t$crmrte) + 
    labs(title = 'Crime Rate vs Wages', x = 'Weekly Wages', y = 'Crimes Committed per Person') +
    geom_smooth(method = 'lm', se = FALSE)
```

An important finding is that the relationship between police per capita and crime rate is positive and moderately large (r = `r round(cor(t$crmrte, t$polpc), 2)`).  This means that either increasing police presence makes crime rate worse or that crime is causing an increase in police presence rather than vice versa.  The latter explanation seems much more logical.  Thus, we will not regress crime rate on police per capita, as the direction of causality is questionable.

```{r crmrte_vs_polpc}
qplot(t$polpc, t$crmrte) + 
    labs(title = 'Crime Rate vs Police Presence', x = 'Police per Capita', y = 'Crimes Committed per Person') +
    geom_smooth(method = 'lm', se = FALSE)
```

Of the three "certainty of punishment" variables, it looks like arrest probability has a moderate effect (r = `r round(cor(t$crmrte, t$prbarr), 2)`) and conviction probability has a weak effect (r = `r round(cor(t$crmrte, t$prbconv), 2)`), but probability of prison sentence has almost no effect (r = `r round(cor(t$crmrte, t$prbpris), 2)`).  It is important to note that these three probabilities seem uncorrelated with one another, so we can include multiple ones in our regression without fear of multicolinearity.  The "severity of punishment" variable, average prison sentence length, does not seem to be correlated with crime rate (r = `r round(cor(t$crmrte, t$avgsen), 2)`).

```{r crmrte_vs_prbarr}
qplot(t$prbarr, t$crmrte) + 
    labs(title = 'Crime Rate vs Arrest Probability', x = 'Arrest Probability', y = 'Crimes Committed per Person') +
    geom_smooth(method = 'lm', se = FALSE)
```
```{r crmrte_vs_prbconv}
qplot(t$prbconv, t$crmrte) + 
    labs(title = 'Crime Rate vs Conviction Probability', x = 'Conviction Probability', y = 'Crimes Committed per Person') +
    geom_smooth(method = 'lm', se = FALSE)
```

Finally, the two demographic variables seem to have relatively weak correlations with crime rate. However, their directions are at least in line with historic sentiment (young male minorities are commonly associated with crime).

```{r crmrte_vs_pctmin80}
qplot(t$pctmin80, t$crmrte) + 
    labs(title = 'Crime Rate vs Percent Minority', x = 'Percent Minority', y = 'Crimes Committed per Person') +
    geom_smooth(method = 'lm', se = FALSE)
```
```{r crmrte_vs_pctymle}
qplot(t$pctymle, t$crmrte) + 
    labs(title = 'Crime Rate vs Percent Young Male', x = 'Percent Young Male', y = 'Crimes Committed per Person') +
    geom_smooth(method = 'lm', se = FALSE)
```


## Dummy Variables

Next, we examine the effect of the three dummy indicators.  First we see if there is a difference in crime rate between non-urban and urban counties.

```{r urban}
ggplot(t, aes(crmrte)) + 
    geom_histogram() + 
    facet_grid(. ~ urban) + 
    theme(panel.spacing = unit(2, "lines")) +
    labs(title = 'Non-urban vs Urban Crime Rate', x = 'Crime Rate')
```
We see that there are only `r sum(t$urban == 'Urban')` counties coded as urban, which is probably too few to make any sweeping inferences.  We will only mention in passing that the crime rate in urban counties does look higher than that in non-urban counties.

Next we examine the differences in geographic region.

```{r region}
ggplot(t, aes(crmrte)) + 
    geom_histogram() + 
    facet_grid(west ~ central) + 
    theme(panel.spacing = unit(1, "lines")) +
    labs(title = 'Crime Rate by Region', x = 'Crime Rate')
```
Again we notice a sparcity in data; this time there are only `r sum(t$west == 'West')` western counties, with a mere single county in the western central area.  However, we do see a relatively even division between central and outer counties, so we will run a t-test to see if there is any difference in crime rate between the two.  

```{r t-test}
t.test(t[t$central == 'Outer', ]$crmrte, 
       t[t$central == 'Central', ]$crmrte)
```
With a p-value of 0.16, we fail to reject the null hypothesis that there is difference in crime rate between central and outer counties.

# Model Building

We will now proceed to build several ordinary least squares (OLS) regression models of crime rate.  We will be reporting heteroskedasticity robust standard errors.

First, we examine whether combining the wages was a prudent choice.

```{r seHC}
# function for getting heteroskedasticity robust standard errors
seHC = function(...) {
    lapply(list(...), function(x) sqrt(diag(vcovHC(x))))
}
```

```{r wage_models, results='asis'}
m1_wage = lm(t$crmrte ~ t$wfed)
m2_wage = lm(t$crmrte ~ t$wcon + t$wtuc + t$wtrd + t$wfir + t$wser + t$wmfg + t$wfed + t$wsta + t$wloc)
m3_wage = lm(t$crmrte ~ t$wage)

stargazer(m1_wage, m2_wage, m3_wage, type = 'latex',
          omit.stat = c('f', 'n'),
          se = seHC(m1_wage, m2_wage, m3_wage),
          star.cutoffs = c(0.05, 0.01, 0.001),
          dep.var.labels = c('Crime Rate'),
          header = FALSE, 
          float = FALSE,
          title = 'Crime Rate Regressed on Wage Variables',
          covariate.labels = c('Construction', 'Trans, Util, Commun', 'Whlesle, Retail, Trade', 
                               'Fin, Ins, Real Est', 'Service', 'Manuacturing', 'Federal', 'State', 
                               'Local', 'Total Sum')
)
```
We see from the above regression table that including each individual wage variable in the regression only provides a small improvement in adjusted $R^2$ from including just the federal wages.  It also causes all the coefficients to lose signifance.  When we combine all the wages into a sum, we see that the adjusted $R^2$ improves more and we end up with a single highly-significant coefficient.  Thus, the total wage variable is a parsimonious way to model the wage effect.

Now we will proceed to build models with all the other variables.  Note that we will not regress on police per capita, as we think that it absorbs some of the causal effect.  

```{r models, results='asis'}
m1 = lm(t$crmrte ~ t$density + t$prbconv)
m2 = lm(t$crmrte ~ t$density + t$prbarr + t$prbconv + t$taxpc + t$pctmin80 + t$pctymle)
m3 = lm(t$crmrte ~ t$prbarr + t$prbconv + t$prbpris + t$avgsen + t$density + t$taxpc + t$pctmin80 + t$mix + t$pctymle + t$wage)

stargazer(m1, m2, m3, type = 'latex',
          omit.stat = c('f', 'n'),
          se = seHC(m1, m2, m3),
          star.cutoffs = c(0.05, 0.01, 0.001),
          header = FALSE, 
          float = FALSE,
          dep.var.labels = c('Crime Rate'),
          title = 'Crime Rate Regressed on Other Variables',
          covariate.labels = c('Population Density', 'Arrest Probability', 'Conviction Probability',
                               'Prison Probability', 'Average Prison Sentence', 'Tax per Capita',
                               'Percent Minority', 'Offense Mix', 'Percent Young Male',
                               'Sum of Wages')
)
```
For model 1, we included only the explanatory variables of key interest.  In this case we picked density and conviction probability because they were both relatively correlated with crime rate, but not correlated with each other.  We see from the regression table that density has a highly significant coefficient but conviction probablity does not.  We have already explained over 50% of the variation in crime rate with these two variables alone (probably mostly from density).

For model 2, we added in variables that increase the accuracy of our result without introducing substantial bias.  For model 3, we added the remaining variables.

# Omitted Variables

We identified seven omitted variables that may introduce bias to the crime rate outcome. The seven variables are a person's morals (Morals), a healthy diet (Diet), a person's mental health (MH), a person's happiness (Happiness), a person's family stability (FS), the amount of drugs in the area (Drugs), and the probability a person will report a crime (prbrc). 

The table below shows omitted variables' effect on both the measure variables and the outcome (crime rate). A value or (1) represents that the omitted variable has a positive correlation with the measured or outcome variable, a (-1) represents that the omitted variable has a negative correlation with the measured or outcome variable, and a (0) represents the omitted variable has no impact on the measured or outcome variable.   


                                  
Omitted Variable   Morals   Diet   MH    Happiness     FS    Drugs   prbrc
-----------------  -------  ----- ----  -----------   ----  ------  ------
crmrate (B1)       -1        0     -1     -1           -1     1       1  
prbarr             -1        0     -1     -1           -1     1       1  
prbconv            -1        0     -1     -1           -1     1       1    
density             0        -1     0      0            0     1       1 
taxpc               0        1      1      1            0     0       0 
pctmin80            0        0      0      0            0     0       0 
pctymle             0        -1     0      0            0     1       0 


The equation for model_2 is:
$$ crmrate = \beta_0 + \beta_1\cdot prbarr + \beta_2\cdot prconv+ \beta_3\cdot density+ \beta_4\cdot taxpc+ \beta_5\cdot pctymle+ \beta_6\cdot pctymle + error$$
$$ Omitted \ variables = Morals, Diet, MH, Happiness, FS, Drugs, and \ prbrc $$
As shown in the table above, the first row displays the impact the omitted variables have on the outcome variable crmrate.

###Morals omitted

$$ B_1 = (-) $$
$$ Morals= \alpha_0 + \alpha_1\cdot prbarr + \alpha_2\cdot prbcov $$
$$ \alpha_1\cdot prbarr>0  \ and \  \alpha_2\cdot prbcov>0 $$ 
The OLS coeffecient will be less negative, therefore losing statistical significance. The OLS coeffecient was overestimated.

###Mental Health (MH) Omitted
$$ B_1 = (-) $$
$$ Mental \  Health= \alpha_0 + \alpha_1\cdot prbarr + \alpha_2\cdot prbcov + \alpha_3\cdot taxpc $$
$$ \alpha_1\cdot prbarr>0  \ and \  \alpha_2\cdot prbcov>0 \ and \  \alpha_3\cdot taxpc>0$$ 
The OLS coeffecient will be less negative, therefore losing statistical significance.The OLS coeffecient was overestimated.

###Happiness Omitted

$$ B_1 = (-) $$
$$ Happiness= \alpha_0 + \alpha_1\cdot prbarr + \alpha_2\cdot prbcov + \alpha_3\cdot taxpc $$
$$ \alpha_1\cdot prbarr>0  \ and \  \alpha_2\cdot prbcov>0 \ and \  \alpha_3\cdot taxpc>0$$ 
The OLS coeffecient will be less negative, therefore losing statistical significance. The OLS coeffecient was overestimated.

###Family Stability (FS) Omitted

$$ B_1 = (-) $$
$$ Family \ Stability= \alpha_0 + \alpha_1\cdot prbarr + \alpha_2\cdot prbcov $$
$$ \alpha_1\cdot prbarr>0  \ and \  \alpha_2\cdot prbcov>0 $$ 

The OLS coeffecient will be less negative, therefore losing statistical significance.

### Drugs in area (Drugs) Omitted

$$ B_1 = (+) $$


$$ Drugs= \alpha_0 + \alpha_1\cdot prbarr + \alpha_2\cdot prbcov + \alpha_3\cdot density+ \alpha_4\cdot pctymle $$
$$\alpha_1\cdot prbarr<0 \ and\  \alpha_2\cdot prbcov<0 \ and\ \alpha_3\cdot density>0 \ and\  \alpha_4\cdot pctymle>0$$
The OLS coeffecient will be more positive, therefore gaining statistical significance. 

### Probability of Reported Crimes (prbrc) Omitted

$$ B_1 = (+) $$

$$ prbrc= \alpha_0 + \alpha_1\cdot prbarr + \alpha_2\cdot prbcov + \alpha_3\cdot density$$
$$ \alpha_1\cdot prbarr<0  \ and \  \alpha_2\cdot prbcov<0 \ and \  \alpha_3\cdot density>0 $$ 

The OLS coeffecient will be more positive, therefore gaining statistical significance. 
 


# Conclusion

We found that the population density is the best predictor we have available for crime rate.  However, it is unlikely that any political platform could make a direct effect to the how closely people live together.  The other variable more tractable.  Of the three probabilities of punishment (arrest, conviction, sentence), arrest has the highest impact on crime rate.  Simply increasing police per capita does not seem to have an effect of decreasing crime rate significantly.  It may be more prudent to dedicate more resources for the existing police force so that more arrests can be made without necessarily increasing police presence.