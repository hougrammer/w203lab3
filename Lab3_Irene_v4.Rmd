---
title: "Lab 3"
author: "David Hou, Scott Hungerfield, Irene Seo"
date: "March 20, 2018"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, include = TRUE, warning = FALSE, message = FALSE, dev = 'pdf', fig.height = 4)
library(dplyr)
library(ggplot2)
library(ggfortify) # autoplot
library(GGally) # ggcorr
library(ggpubr) # ggarrange
library(lmtest)
library(sandwich)
library(stargazer)
```

# Introduction

The purpose of this study is to provide information for a political campaign in North Carolina.  Specifically, we want to determine what variables contribute to crime rate and help the campaign propose policy suggestions to local governments.  To accomplish this, we were given crime data from several North Carolina counties along with other variables.  We will run ordinary least square regressions to help determine which of these are the best predictors of crime.

# Data Cleaning

First we need to clean the data.  In the raw data, we notice that that the last 6 rows are empty.  The integer columns are probably more useful to us as factors.  The prbconv is coded as a factor, so we turn it into a numeric.

We also notice that prbarr and prbconv have values that are greater than 1, which does not make much sense because they are probability variables.  Specifically, we find nine cases where prbconv is greater than 1 and one case where both are greater than one.  We create a badprb flag which is set to 1 for the former nine cases and 2 for singular latter case.  We also create a second data table, where all the questionable probabilities are removed.

As a minor change, we divide pctmin80 by 100, so that it matches the formatting of pctymle.  Both variables are percentages and we've arbitrarily chosen to represent them as a number between 0 and 1 rather than 0 to 100.

```{r data_cleaning}
raw = as_tibble(read.csv('crime_v2.csv'))

t = raw %>% 
    filter(!is.na(county)) %>%
    mutate(prbconv = as.numeric(as.character(prbconv))) %>%
    mutate(pctmin80 = pctmin80 / 100) %>%
    mutate_if(is.integer, as.factor) %>%
    mutate(badprb = as.factor((prbarr > 1) + (prbconv > 1)))
levels(t$west) = c('East', 'West')
t$west = relevel(t$west, 'West') # Put West first so it appears on the left on facet plots 
levels(t$central) = c('Outer', 'Central')
levels(t$urban) = c('Non-urban', 'Urban')
levels(t$badprb) = c('Normal', 'prbconv > 1', 'prbarr > 1 and prbconv > 1')

t2 = t %>% 
    filter(prbarr < 1 & prbconv < 1)
```


Here is a summary of the data (with the questionable probabilities left in).
```{r summary, results='asis'}
stargazer(data.frame(t), type = 'latex', nobs = FALSE, header = FALSE, float = FALSE)
```

# Examining Key Variables of Interest

## Metric Variables

We start our analysis by first looking at the metric variables, i.e. all the variables less county, year, west, central, and urban.  Crime rate is our most important variable as it is the output that we are trying to study.

```{r crmrte_hist, fig.height=3}
qplot(t$crmrte, col = I('white')) + 
    labs(title = 'Crime Rate', x = 'Crimes Committed per Person')
```

We see that crime rate has some positive skew, but does not seem to have a very exotic distribution.  To determine which variables are of interest to us when predicting crime rate, we look at the correlation matrices among the variables.  Since a large portion of dataset deals with wage, let us first examine those variables by themselves.  We first take a look at the correlation matrix among them and crime rate.

```{r wage_cor, fig.height=5}
ggcorr(t %>% select(crmrte, wcon, wtuc, wtrd, wfir, wser, wmfg, wfed, wsta, wloc),
       label = TRUE, label_round = 2, label_size = 3, size = 3) + 
    ggtitle('Correlation Matrix of Crime Rate and Wages')
```
Surprisingly, we find that crime rate is actually positively correlated with all wages except from the service industry.  This seems counter to common sentiment that crime is more prevalent in low income areas.

For ease of comparison with the other variables, we create a new one that is the sum of all the other wages.  We will see later that this data transformation does not make a large difference in the regression analysis.

```{r correlation_matrix, fig.height=5}
t = t %>% mutate(wage = wcon + wtuc + wtrd + wfir + wser + wmfg + wfed + wsta + wloc)
t2 = t2 %>% mutate(wage = wcon + wtuc + wtrd + wfir + wser + wmfg + wfed + wsta + wloc)

ggcorr(t %>% select(crmrte, prbarr, prbconv, prbpris, avgsen, polpc, density, taxpc, pctmin80, mix, pctymle, wage),
       label = TRUE, label_round = 2, label_size = 3, size = 3) + 
    ggtitle('Correlation Matrix')
```

From the correlation matrix, we see that population density stands out as being highly correlated with crime rate (r = `r round(cor(t$crmrte, t$density), 2)`).  This variable looks like a good candidate as a predictor for crime rate.  One explanation could be that as more people move into an area, the increased number of interactions give opportunity for more crime.  In addition, more people in an area probably increases the chance that crime will actually be seen.

The other two variables with moderately positive correlation are tax per capita (r = `r round(cor(t$crmrte, t$taxpc), 2)`) and total wages (r = `r round(cor(t$crmrte, t$wage), 2)`).  Note that population density is weakly correlated with taxes (r = `r round(cor(t$taxpc, t$density), 2)`) and moderately correlated with wages (r = `r round(cor(t$wage, t$density), 2)`).  We believe that taxes and wages are not directly causing higher crime rates but are rising along with crime rate because they are rising along with density.  Also, it is interesting that taxes and wages are not very correlated with themselves (r = `r round(cor(t$taxpc, t$wage), 2)`).  This finding is surprising, as one would expect that wages and taxes would go up very closely with each other.  Along with the questionable probability numbers, we are left to question the integrity of this dataset.  At the minimum, we desire some extra explanation as to how the data were taken. 

An important finding is that the relationship between police per capita and crime rate is positive (r = `r round(cor(t$crmrte, t$polpc), 2)`).  This means that either increasing police presence makes crime rate worse or that crime is causing an increase in police presence rather than vice versa.  The latter explanation seems much more logical.  Thus, we will not regress crime rate on police per capita, as the direction of causality is questionable.

Of the three "certainty of punishment" variables, it looks like arrest probability has a moderate effect (r = `r round(cor(t$crmrte, t$prbarr), 2)`) and conviction probability has a weak effect (r = `r round(cor(t$crmrte, t$prbconv), 2)`), but probability of prison sentence has almost no effect (r = `r round(cor(t$crmrte, t$prbpris), 2)`).  It is important to note that these three probabilities seem uncorrelated with one another, so we can include multiple ones in our regression without fear of multicolinearity.  The "severity of punishment" variable, average prison sentence length, does not seem to be correlated with crime rate (r = `r round(cor(t$crmrte, t$avgsen), 2)`).

Finally, the two demographic variables seem to have relatively weak correlations with crime rate. However, their directions are at least in line with historic sentiment (young male minorities are commonly associated with crime).

Below are the bivariate scatter plots between crime rate and each of the input variables.  The linear regression lines are shown on the plots for convenience but are not meant to be rigorous models at this point.  We have also divided the variables up into two rough groups: one dealing directly dealing with law enforcement and one dealing with socioeconomic/demographic factors.

### Law Enforcement Variables
```{r law_enforcement_plots, fig.height=9}
p1 = qplot(t$prbarr, t$crmrte, col = t$badprb) + 
    labs(title = NULL, col = NULL, x = 'Arrest Probability', y = 'Crimes Committed per Person') +
    geom_smooth(method = 'lm', se = FALSE)
p2 = qplot(t$prbconv, t$crmrte, col = t$badprb) + 
    labs(title = NULL, col = NULL, x = 'Conviction Probability', y = 'Crimes Committed per Person') +
    geom_smooth(method = 'lm', se = FALSE)
p3 = qplot(t$prbpris, t$crmrte, col = t$badprb) + 
    labs(title = NULL, col = NULL, x = 'Prison Probability', y = 'Crimes Committed per Person') +
    geom_smooth(method = 'lm', se = FALSE)
p4 = qplot(t$avgsen, t$crmrte, col = t$badprb) + 
    labs(title = NULL, col = NULL, x = 'Average Sentence Length', y = 'Crimes Committed per Person') +
    geom_smooth(method = 'lm', se = FALSE)
p5 = qplot(t$polpc, t$crmrte, col = t$badprb) + 
    labs(title = NULL, col = NULL, x = 'Police per Capita', y = 'Crimes Committed per Person') +
    geom_smooth(method = 'lm', se = FALSE)
p6 = qplot(t$mix, t$crmrte, col = t$badprb) + 
    labs(title = NULL, col = NULL, x = 'Percent Face-to-Face Crime', y = 'Crimes Committed per Person') +
    geom_smooth(method = 'lm', se = FALSE)
ggarrange(p1, p2, p3, p4, p5, p6, nrow = 3, ncol = 2, common.legend = T)
```

Immediately, we notice that the counties with anomalous conviction probabilities are problematic.  Instead of being uniformly distributed among the data, they are all very good in terms of having low crime rate.  County 115 (Madison County [^1]) is the point with both arrest probability and conviction probability greater than 1.  It has the lowest crime rate among all counties.  Madison County also has the longest average prison sentence length and most police per capita by far.  In essence, Madison County seems like one in which law enforcement is extremely strict and has very low crime rate as a result.

[^1]: We assume the county numbers in the dataset are Federal Processing Standard Publication (FIPS) numbers.  The North Carolina FIPS codes were found at https://en.wikipedia.org/wiki/List_of_counties_in_North_Carolina. 
### Socioeconomic/Demographic Variables

```{r socioeconomic_plots, fig.height=9}
p7 = qplot(t$density, t$crmrte, col = t$badprb) + 
    labs(title = NULL, col = NULL, x = 'Population Density', y = 'Crimes Committed per Person') +
    geom_smooth(method = 'lm', se = FALSE)
p8 = qplot(t$taxpc, t$crmrte, col = t$badprb) + 
    labs(title = NULL, col = NULL, x = 'Tax per Capita', y = 'Crimes Committed per Person') +
    geom_smooth(method = 'lm', se = FALSE)
p9 = qplot(t$pctmin80, t$crmrte, col = t$badprb) + 
    labs(title = NULL, col = NULL, x = 'Percent Minority (1980)', y = 'Crimes Committed per Person') +
    geom_smooth(method = 'lm', se = FALSE)
p10 = qplot(t$pctymle, t$crmrte, col = t$badprb) + 
    labs(title = NULL, col = NULL, x = 'Percent Young Male', y = 'Crimes Committed per Person') +
    geom_smooth(method = 'lm', se = FALSE)
p11 = qplot(t$wage, t$crmrte, col = t$badprb) + 
    labs(title = NULL, col = NULL, x = 'Sum of Wages', y = 'Crimes Committed per Person') +
    geom_smooth(method = 'lm', se = FALSE)
ggarrange(p7, p8, p9, p10, p11, nrow = 3, ncol = 2, common.legend = T)
```

Again, we notice the problematic high punishment probability counties.  This time, we find Madison County is very low in population density, tax per capita, minority population, and percent young male.  Indeed, the county is sparsely populated and located in the Appalachian Mountains, within heavy forests.

[^2]: This information from https://en.wikipedia.org/wiki/Madison_County,_North_Carolina.

## Dummy Variables

Next, we examine the effect of the three dummy indicators.  First we see if there is a difference in crime rate between non-urban and urban counties.  As an aside, we are assuming "non-urban" refers to a combination of suburb-dominated and rural areas.  We are not sure if this geographical assumption is actually true in North Carolina.

```{r urban}
ggplot(t, aes(crmrte)) + 
    geom_histogram() + 
    facet_grid(. ~ urban) + 
    theme(panel.spacing = unit(2, "lines")) +
    labs(title = 'Non-urban vs Urban Crime Rate', x = 'Crime Rate')
```
We see that there are only `r sum(t$urban == 'Urban')` counties coded as urban, which is probably too few to make any sweeping inferences.  We will only mention in passing that the crime rate in urban counties does look higher than that in non-urban counties.

Next we examine the differences in geographic region.

```{r region}
ggplot(t, aes(crmrte)) + 
    geom_histogram() + 
    facet_grid(west ~ central) + 
    theme(panel.spacing = unit(1, "lines")) +
    labs(title = 'Crime Rate by Region', x = 'Crime Rate')
```
Again we notice a sparcity in data; this time there are only `r sum(t$west == 'West')` western counties, with a mere single county in the western central area.  However, we do see a relatively even division between central and outer counties, so we will run a t-test to see if there is any difference in crime rate between the two.  

```{r t-test}
t.test(t[t$central == 'Outer', ]$crmrte, 
       t[t$central == 'Central', ]$crmrte)
```
With a p-value of 0.16, we fail to reject the null hypothesis that there is difference in crime rate between central and outer counties.

# Model Building

We will now proceed to build several ordinary least squares (OLS) regression models of crime rate.  We will be reporting heteroskedasticity robust standard errors.

## Wage Transformation
First, we examine whether combining the wages was a prudent choice.

```{r seHC}
# function for getting heteroskedasticity robust standard errors
seHC = function(...) {
    lapply(list(...), function(x) sqrt(diag(vcovHC(x))))
}
```

```{r wage_models, results='asis'}
m1_wage = lm(t$crmrte ~ t$wfed)
m2_wage = lm(t$crmrte ~ t$wcon + t$wtuc + t$wtrd + t$wfir + t$wser + t$wmfg + t$wfed + t$wsta + t$wloc)
m3_wage = lm(t$crmrte ~ t$wage)

stargazer(m1_wage, m2_wage, m3_wage, type = 'latex',
          omit.stat = c('f', 'n'),
          se = seHC(m1_wage, m2_wage, m3_wage),
          star.cutoffs = c(0.05, 0.01, 0.001),
          dep.var.labels = c('Crime Rate'),
          header = FALSE, 
          float = FALSE,
          title = 'Crime Rate Regressed on Wage Variables',
          covariate.labels = c('Construction', 'Trans, Util, Commun', 'Whlesle, Retail, Trade', 
                               'Fin, Ins, Real Est', 'Service', 'Manuacturing', 'Federal', 'State', 
                               'Local', 'Total Sum')
)
```
We see from the above regression table that including each individual wage variable in the regression only provides a small improvement in adjusted $R^2$ from including just the federal wages.  It also causes all the coefficients to lose signifance.  When we combine all the wages into a sum, we see that the adjusted $R^2$ improves more and we end up with a single highly-significant coefficient.  Thus, the total wage variable is a parsimonious way to model the wage effect.

Now we will proceed to build models with all the other variables.  Note that we will not regress on police per capita, as we think that it absorbs some of the causal effect.  


## Multivariate OLS

We built linear models by adding key variables that we found to potentially affect crime rates in EDA.
First, for model 1, we included only the explanatory variables of key interest.  In this case we picked density and conviction probability because they were both relatively correlated with crime rate, but not correlated with each other.  We see from the regression table that density has a highly significant coefficient but conviction probablity does not.  We have already explained over 50% of the variation in crime rate with these two variables alone (probably mostly from density).

For model 2, we added in variables that increase the accuracy of our result without introducing substantial bias based on EDA. In addition to density and prbconv variables, we first included variables that showed moderate correlation with crime rates but relatively low correlation with density and probability of conviction, such as probability of arrest, percentage of young male and percentage of minority. Among economic variables, tax per capita and sum of wages, both taxpc and sum of wage have moderate correlation with crime rates - r=`r round(cor(t$crmrte, t$taxpc), 2)` for tax, and r=`r round(cor(t$crmrte, t$wage), 2)` for wage. However, since wage shows much higher correlation with our variables of key interest, having r= `r round(cor(t$density, t$wage), 2)` with density and r= `r round(cor(t$prbarr, t$wage), 2)`, compared to tax per capita, we included taxpc in our model to represent economic effect.

For model 3, we added the remaining variables in order to compare efficiency of the best model we found.

```{r models, results='asis'}
m1 = lm(t$crmrte ~ t$density + t$prbconv)
m2 = lm(t$crmrte ~ t$density + t$prbconv + t$prbarr + t$pctmin80 + t$pctymle + t$taxpc)
m3 = lm(t$crmrte ~ t$density + t$prbconv + t$prbarr + t$pctmin80 + t$pctymle + t$taxpc + t$prbpris + t$avgsen + t$mix + t$wage)

stargazer(m1, m2, m3, type = 'latex',
          omit.stat = c('f', 'n'),
          se = seHC(m1, m2, m3),
          star.cutoffs = c(0.05, 0.01, 0.001),
          header = FALSE, 
          float = FALSE,
          dep.var.labels = c('Crime Rate'),
          title = 'Crime Rate Regressed on Other Variables',
          covariate.labels = c('Population Density', 'Conviction Probability', 'Arrest Probability',
                               'Percent Minority', 'Percent Young Male', 'Tax per Capita',
                               'Prison Probability', 'Average Prison Sentence', 'Offense Mix', 
                               'Sum of Wages')
)
```



## Variable transformation
Even though we don't see the needs for transformations, we transformed some of our variables in order to check how robust our best fit model is.

```{r transformation}
m4 = lm(log(t$crmrte) ~ t$density + t$prbconv + t$taxpc + t$pctmin80 + t$pctymle)
m5 = lm(log(t$crmrte) ~ log(t$density) + log(t$prbconv) + log(t$taxpc) + log(t$pctmin80) + log(t$pctymle))
stargazer(m2, m4, m5, type = 'text',
          se = seHC(m2, m4, m5))
```

## Statistical and Practical Significance

From the regression tables above, we find model2 as our best-fit model with a moderately high $R^2$ at 0.749. In this model, density, percentage of young male, and percentage of minorities have statistical significance. In practical perspective, following variables have more significance in raising crime rates:

1) Percent young male
Percentage of young male has the highest practical significance: As percentage of young male increases by 1, crime rate increases by 0.147. Considering the average crime rates in North Carolina is currently at 0.033, this is a huge impact.

2) Percent minority
Next practically significant variable is percent minority. As percent minority increases by 1, crime rate increases by 0.028.

3) Conviction probability
As conviction probability decreases by 0.012, crime rate increases by 0.01. This is practically significant, considering the positively skewed distribution of crime rates with mean at 0.033.

4) Arrest probability
As arrest probability decreases by 0.026, crime rate increases by 0.01. This is even more practically significant than conviction probability, given that the mean of arrest probability is 0.295.


# Omitted Variables

We identified seven omitted variables that may introduce bias to the crime rate outcome. The seven variables are a person's morals (Morals), a healthy diet (Diet), a person's mental health (MH), a person's happiness (Happiness), a person's family stability (FS), the amount of drugs in the area (Drugs), and the probability a person will report a crime (prbrc). 

The table below shows omitted variables' effect on both the measure variables and the outcome (crime rate). A value or (1) represents that the omitted variable has a positive correlation with the measured or outcome variable, a (-1) represents that the omitted variable has a negative correlation with the measured or outcome variable, and a (0) represents the omitted variable has no impact on the measured or outcome variable. 

Omitted Variable   Morals   Diet   MH   Happiness  FS   Drugs   prbrc
-----------------  -------  -----  ---  ---------  ---  ------  ------
crmrate (B1)       -1        0     -1     -1        -1     1       1  
prbarr             -1        0     -1     -1        -1     1       1  
prbconv            -1        0     -1     -1        -1     1       1    
density             0       -1      0      0         0     1       0 
taxpc               0        1      1      1         0     0       0 
pctmin80            0        0      0      0         0     0       0 
pctymle             0       -1      0      0         0     1       0

The equation for model_2 is:
$$ crmrate = \beta_0 + \beta_1\cdot prbarr + \beta_2\cdot prconv+ \beta_3\cdot density+ \beta_4\cdot taxpc+ \beta_5\cdot pctymle+ \beta_6\cdot pctymle + error$$
$$ Omitted \ variables = Morals, Diet, MH, Happiness, FS, Drugs, and \ prbrc $$
As shown in the table above, the first row displays the impact the omitted variables have on the outcome variable crmrate.

###Morals omitted

$$ B_1 = (-) $$
$$ Morals= \alpha_0 + \alpha_1\cdot prbarr + \alpha_2\cdot prbcov $$
$$ \alpha_1\cdot prbarr>0  \ and \  \alpha_2\cdot prbcov>0 $$ 
The OLS coeffecient will be less negative, therefore losing statistical significance.

###Mental Health (MH) Omitted
$$ B_1 = (-) $$
$$ Mental \  Health= \alpha_0 + \alpha_1\cdot prbarr + \alpha_2\cdot prbcov + \alpha_3\cdot taxpc $$
$$ \alpha_1\cdot prbarr>0  \ and \  \alpha_2\cdot prbcov>0 \ and \  \alpha_3\cdot taxpc>0$$ 
The OLS coeffecient will be less negative, therefore losing statistical significance.

###Happiness Omitted

$$ B_1 = (-) $$
$$ Happiness= \alpha_0 + \alpha_1\cdot prbarr + \alpha_2\cdot prbcov + \alpha_3\cdot taxpc $$
$$ \alpha_1\cdot prbarr>0  \ and \  \alpha_2\cdot prbcov>0 \ and \  \alpha_3\cdot taxpc>0$$ 
The OLS coeffecient will be less negative, therefore losing statistical significance.

###Family Stability (FS) Omitted

$$ B_1 = (-) $$
$$ Family \ Stability= \alpha_0 + \alpha_1\cdot prbarr + \alpha_2\cdot prbcov $$
$$ \alpha_1\cdot prbarr>0  \ and \  \alpha_2\cdot prbcov>0 $$ 

The OLS coeffecient will be less negative, therefore losing statistical significance.

### Drugs in area (Drugs) Omitted

$$ B_1 = (+) $$


$$ Drugs= \alpha_0 + \alpha_1\cdot prbarr + \alpha_2\cdot prbcov + \alpha_3\cdot density+ \alpha_4\cdot pctymle $$
$$\alpha_1\cdot prbarr<0 \ and\  \alpha_2\cdot prbcov<0 \ and\ \alpha_3\cdot density>0 \ and\  \alpha_4\cdot pctymle>0$$
The OLS coeffecient will be more positive, therefore gaining statistical significance. 

### Probability of Reported Crimes (prbrc) Omitted

$$ B_1 = (+) $$

$$ prbrc= \alpha_0 + \alpha_1\cdot prbarr + \alpha_2\cdot prbcov $$
$$ \alpha_1\cdot prbarr<0  \ and \  \alpha_2\cdot prbcov<0 $$ 
The OLS coeffecient will be less positive, therefore losing statistical significance. 


# Conclusion

We found that the population density is the best predictor we have available for crime rate.  However, it is unlikely that any political platform could make a direct effect to the how closely people live together.  The other variable more tractable.  Of the three probabilities of punishment, arrest has the highest impact on crime rate.  Simply increasing police per capita does not seem to be having an effect of decreasing crime rate significantly.  It may be more prudent to dedicate more resources for the existing police force so that more arrests can be made without necessarily increasing police presence.


# Assessment of 6 CLM assumptions
We check all of 6 classical linear model assumptions with our best-fit model - m2.

## Linear population model

## Random Sampling

## Multicollinearity
Since we are able to form a linear model in R, we can say that there is no perfect multicollinearity between variables.

## Zero conditional mean
```{r}
plot(m2, which = 1)
```

Residuals vs. Fitted graph shows that zero-conditional mean is quite biased in our best model, with a high point on the left side. In order to troubleshoot this violation, we can try adding in more variables or change functional form.

```{r}
plot(m4, which = 1)
```

(???)


## Homoskedasticity
```{r}
plot(m2, which = 3)
```

Scale-Location gragh shows that homoskedasticity is violated by the same data point 51 that violated zero-conditional mean. However, this condition can be satisfied with simple data transformation of taking log of dependent variable, crime rates.

```{r}
plot(m4, which = 3)
```

In addition, since we're using heteroskedasticity-robuse standard errors, our model remains robust.

## Normality of Errors
```{r}
plot(m2, which = 2)
```

Normality of Errors is also violated from this Normal Q-Q plot. We can try transformed model again to check if the assumption is satisfied.

```{r}
plot(m4, which = 2)
```

Simple log transformation of dependent variable has moderately fixed this problem. In addition, with the size of dataset at 91 (n > 30), we can rely on asymtotic properties of OLS to show that our model is still robust.